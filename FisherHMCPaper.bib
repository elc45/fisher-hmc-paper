
@article{hoffman_neutra-lizing_2019,
	title = {{NeuTra}-lizing Bad Geometry in Hamiltonian Monte Carlo Using Neural Transport},
	url = {http://arxiv.org/abs/1903.03704},
	abstract = {Hamiltonian Monte Carlo is a powerful algorithm for sampling from diﬃcult-to-normalize posterior distributions. However, when the geometry of the posterior is unfavorable, it may take many expensive evaluations of the target distribution and its gradient to converge and mix. We propose neural transport ({NeuTra}) {HMC}, a technique for learning to correct this sort of unfavorable geometry using inverse autoregressive ﬂows ({IAF}), a powerful neural variational inference technique. The {IAF} is trained to minimize the {KL} divergence from an isotropic Gaussian to the warped posterior, and then {HMC} sampling is performed in the warped space. We evaluate {NeuTra} {HMC} on a variety of synthetic and real problems, and ﬁnd that it significantly outperforms vanilla {HMC} both in time to reach the stationary distribution and asymptotic eﬀective-sample-size rates.},
	journaltitle = {{arXiv}:1903.03704 [stat]},
	author = {Hoffman, Matthew and Sountsov, Pavel and Dillon, Joshua V. and Langmore, Ian and Tran, Dustin and Vasudevan, Srinivas},
	urldate = {2022-04-11},
	date = {2019-03-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1903.03704},
	keywords = {Statistics - Computation, Statistics - Machine Learning},
	file = {Hoffman et al. - 2019 - NeuTra-lizing Bad Geometry in Hamiltonian Monte Ca.pdf:/home/adr/Zotero/storage/3H88E6WL/Hoffman et al. - 2019 - NeuTra-lizing Bad Geometry in Hamiltonian Monte Ca.pdf:application/pdf},
}

@misc{langmore_condition_2020,
	title = {A Condition Number for Hamiltonian Monte Carlo},
	url = {http://arxiv.org/abs/1905.09813},
	abstract = {Hamiltonian Monte Carlo is a popular sampling technique for smooth target densities. The scale lengths of the target have long been known to inﬂuence integration error and sampling eﬃciency. However, quantitative measures intrinsic to the target have been lacking. In this paper, we restrict attention to the multivariate Gaussian and the leapfrog integrator, and obtain a condition number corresponding to sampling eﬃciency. This number, based on the spectral and Schatten norms, quantiﬁes the number of leapfrog steps needed to eﬃciently sample. We demonstrate its utility by using this condition number to analyze {HMC} preconditioning techniques. We also ﬁnd the condition number of large inverse Wishart matrices, from which we derive burn-in heuristics.},
	number = {{arXiv}:1905.09813},
	publisher = {{arXiv}},
	author = {Langmore, Ian and Dikovsky, Michael and Geraedts, Scott and Norgaard, Peter and Von Behren, Rob},
	urldate = {2022-10-16},
	date = {2020-02-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.09813 [math, stat]},
	keywords = {90-08, G.3, Mathematics - Statistics Theory, Statistics - Computation, Statistics - Methodology},
	file = {Langmore et al. - 2020 - A Condition Number for Hamiltonian Monte Carlo.pdf:/home/adr/Zotero/storage/NV6BYFSN/Langmore et al. - 2020 - A Condition Number for Hamiltonian Monte Carlo.pdf:application/pdf},
}

@misc{yang_variational_2019,
	title = {Variational approximations using Fisher divergence},
	url = {http://arxiv.org/abs/1905.05284},
	abstract = {Modern applications of Bayesian inference involve models that are suﬃciently complex that the corresponding posterior distributions are intractable and must be approximated. The most common approximation is based on Markov chain Monte Carlo, but these can be expensive when the data set is large and/or the model is complex, so more eﬃcient variational approximations have recently received considerable attention. The traditional variational methods, that seek to minimize the Kullback–Leibler divergence between the posterior and a relatively simple parametric family, provide accurate and eﬃcient estimation of the posterior mean, but often does not capture other moments, and have limitations in terms of the models to which they can be applied. Here we propose the construction of variational approximations based on minimizing the Fisher divergence, and develop an eﬃcient computational algorithm that can be applied to a wide range of models without conjugacy or potentially unrealistic mean-ﬁeld assumptions. We demonstrate the superior performance of the proposed method for the benchmark case of logistic regression.},
	number = {{arXiv}:1905.05284},
	publisher = {{arXiv}},
	author = {Yang, Yue and Martin, Ryan and Bondell, Howard},
	urldate = {2022-10-16},
	date = {2019-05-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.05284 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
	file = {Yang et al. - 2019 - Variational approximations using Fisher divergence.pdf:/home/adr/Zotero/storage/F4W9ETZB/Yang et al. - 2019 - Variational approximations using Fisher divergence.pdf:application/pdf},
}

@misc{costa_fisher_2014,
	title = {Fisher information distance: a geometrical reading},
	url = {http://arxiv.org/abs/1210.2354},
	shorttitle = {Fisher information distance},
	abstract = {This paper is a strongly geometrical approach to the Fisher distance, which is a measure of dissimilarity between two probability distribution functions. The Fisher distance, as well as other divergence measures, are also used in many applications to establish a proper data average. The main purpose is to widen the range of possible interpretations and relations of the Fisher distance and its associated geometry for the prospective applications. It focuses on statistical models of the normal probability distribution functions and takes advantage of the connection with the classical hyperbolic geometry to derive closed forms for the Fisher distance in several cases. Connections with the well-known Kullback-Leibler divergence measure are also devised.},
	number = {{arXiv}:1210.2354},
	publisher = {{arXiv}},
	author = {Costa, Sueli I. R. and Santos, Sandra A. and Strapasson, João E.},
	urldate = {2023-09-11},
	date = {2014-01-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1210.2354 [math-ph, stat]},
	keywords = {Computer Science - Information Theory, Mathematical Physics, Statistics - Methodology},
	file = {Costa et al. - 2014 - Fisher information distance a geometrical reading.pdf:/home/adr/Zotero/storage/LKAT8IA7/Costa et al. - 2014 - Fisher information distance a geometrical reading.pdf:application/pdf},
}

@misc{neal_mcmc_2012,
	title = {{MCMC} using Hamiltonian dynamics},
	url = {http://arxiv.org/abs/1206.1901},
	doi = {10.48550/arXiv.1206.1901},
	abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard-to-compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, I discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for deciding on acceptance or rejection, computing trajectories using fast approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories from taking much computation time.},
	number = {{arXiv}:1206.1901},
	publisher = {{arXiv}},
	author = {Neal, Radford M.},
	urldate = {2024-11-26},
	date = {2012-06-09},
	eprinttype = {arxiv},
	eprint = {1206.1901},
	keywords = {Physics - Computational Physics, Statistics - Computation},
	file = {Preprint PDF:/home/adr/Zotero/storage/ZJD3J6B5/Neal - 2012 - MCMC using Hamiltonian dynamics.pdf:application/pdf;Snapshot:/home/adr/Zotero/storage/62MFFFFN/1206.html:text/html},
}

@misc{tran_tuning_2024,
	title = {Tuning diagonal scale matrices for {HMC}},
	url = {http://arxiv.org/abs/2403.07495},
	doi = {10.48550/arXiv.2403.07495},
	abstract = {Three approaches for adaptively tuning diagonal scale matrices for {HMC} are discussed and compared. The common practice of scaling according to estimated marginal standard deviations is taken as a benchmark. Scaling according to the mean log-target gradient ({ISG}), and a scaling method targeting that the frequency of when the underlying Hamiltonian dynamics crosses the respective medians should be uniform across dimensions, are taken as alternatives. Numerical studies suggest that the {ISG} method leads in many cases to more efficient sampling than the benchmark, in particular in cases with strong correlations or non-linear dependencies. The {ISG} method is also easy to implement, computationally cheap and would be relatively simple to include in automatically tuned codes as an alternative to the benchmark practice.},
	number = {{arXiv}:2403.07495},
	publisher = {{arXiv}},
	author = {Tran, Jimmy Huy and Kleppe, Tore Selland},
	urldate = {2024-11-26},
	date = {2024-03-12},
	eprinttype = {arxiv},
	eprint = {2403.07495},
	keywords = {Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
	file = {Preprint PDF:/home/adr/Zotero/storage/LSI9MEZW/Tran and Kleppe - 2024 - Tuning diagonal scale matrices for HMC.pdf:application/pdf;Snapshot:/home/adr/Zotero/storage/EJ48D3WM/2403.html:text/html},
}

@incollection{nielsen_riemannian_2013,
	location = {Berlin, Heidelberg},
	title = {The Riemannian Mean of Positive Matrices},
	isbn = {978-3-642-30231-2 978-3-642-30232-9},
	url = {https://link.springer.com/10.1007/978-3-642-30232-9_2},
	pages = {35--51},
	booktitle = {Matrix Information Geometry},
	publisher = {Springer Berlin Heidelberg},
	author = {Bhatia, Rajendra},
	editor = {Nielsen, Frank and Bhatia, Rajendra},
	urldate = {2024-11-26},
	date = {2013},
	langid = {english},
	doi = {10.1007/978-3-642-30232-9_2},
	file = {PDF:/home/adr/Zotero/storage/JF3GN5TD/Bhatia - 2013 - The Riemannian Mean of Positive Matrices.pdf:application/pdf},
}

@misc{dinh_density_2017,
	abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
	annote = {Comment: 10 pages of main content, 3 pages of bibliography, 18 pages of appendix. Accepted at ICLR 2017},
	author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
	doi = {10.48550/arXiv.1605.08803},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	month = feb,
	note = {arXiv:1605.08803 [cs]},
	publisher = {arXiv},
	title = {Density estimation using {Real} {NVP}},
	url = {http://arxiv.org/abs/1605.08803},
	urldate = {2025-03-21},
	year = {2017}
}

@article{carpenter_stan_2017,
  title = {Stan: A Probabilistic Programming Language},
  author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Riddell, Andrew},
  year = {2017},
  journal = {Journal of Statistical Software},
  volume = {76},
  number = {1},
  pages = {1--32},
  doi = {10.18637/jss.v076.i01}
}

@misc{hoffman_nuts_2011,
      title={The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo}, 
      author={Matthew D. Hoffman and Andrew Gelman},
      year={2011},
      eprint={1111.4246},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url={https://arxiv.org/abs/1111.4246}, 
}

@misc{magnusson_posteriordb_2024,
      title={posteriordb: Testing, Benchmarking and Developing Bayesian Inference Algorithms}, 
      author={Måns Magnusson and Jakob Torgander and Paul-Christian Bürkner and Lu Zhang and Bob Carpenter and Aki Vehtari},
      year={2024},
      eprint={2407.04967},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url={https://arxiv.org/abs/2407.04967}, 
}