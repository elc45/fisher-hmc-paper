
@article{hoffman_neutra-lizing_2019,
	title = {{NeuTra}-lizing Bad Geometry in Hamiltonian Monte Carlo Using Neural Transport},
	url = {http://arxiv.org/abs/1903.03704},
	abstract = {Hamiltonian Monte Carlo is a powerful algorithm for sampling from diﬃcult-to-normalize posterior distributions. However, when the geometry of the posterior is unfavorable, it may take many expensive evaluations of the target distribution and its gradient to converge and mix. We propose neural transport ({NeuTra}) {HMC}, a technique for learning to correct this sort of unfavorable geometry using inverse autoregressive ﬂows ({IAF}), a powerful neural variational inference technique. The {IAF} is trained to minimize the {KL} divergence from an isotropic Gaussian to the warped posterior, and then {HMC} sampling is performed in the warped space. We evaluate {NeuTra} {HMC} on a variety of synthetic and real problems, and ﬁnd that it significantly outperforms vanilla {HMC} both in time to reach the stationary distribution and asymptotic eﬀective-sample-size rates.},
	journaltitle = {{arXiv}:1903.03704 [stat]},
	author = {Hoffman, Matthew and Sountsov, Pavel and Dillon, Joshua V. and Langmore, Ian and Tran, Dustin and Vasudevan, Srinivas},
	urldate = {2022-04-11},
	date = {2019-03-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1903.03704},
	keywords = {Statistics - Computation, Statistics - Machine Learning},
	file = {Hoffman et al. - 2019 - NeuTra-lizing Bad Geometry in Hamiltonian Monte Ca.pdf:/home/adr/Zotero/storage/3H88E6WL/Hoffman et al. - 2019 - NeuTra-lizing Bad Geometry in Hamiltonian Monte Ca.pdf:application/pdf},
}

@misc{langmore_condition_2020,
	title = {A Condition Number for Hamiltonian Monte Carlo},
	url = {http://arxiv.org/abs/1905.09813},
	abstract = {Hamiltonian Monte Carlo is a popular sampling technique for smooth target densities. The scale lengths of the target have long been known to inﬂuence integration error and sampling eﬃciency. However, quantitative measures intrinsic to the target have been lacking. In this paper, we restrict attention to the multivariate Gaussian and the leapfrog integrator, and obtain a condition number corresponding to sampling eﬃciency. This number, based on the spectral and Schatten norms, quantiﬁes the number of leapfrog steps needed to eﬃciently sample. We demonstrate its utility by using this condition number to analyze {HMC} preconditioning techniques. We also ﬁnd the condition number of large inverse Wishart matrices, from which we derive burn-in heuristics.},
	number = {{arXiv}:1905.09813},
	publisher = {{arXiv}},
	author = {Langmore, Ian and Dikovsky, Michael and Geraedts, Scott and Norgaard, Peter and Von Behren, Rob},
	urldate = {2022-10-16},
	date = {2020-02-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.09813 [math, stat]},
	keywords = {90-08, G.3, Mathematics - Statistics Theory, Statistics - Computation, Statistics - Methodology},
	file = {Langmore et al. - 2020 - A Condition Number for Hamiltonian Monte Carlo.pdf:/home/adr/Zotero/storage/NV6BYFSN/Langmore et al. - 2020 - A Condition Number for Hamiltonian Monte Carlo.pdf:application/pdf},
}

@misc{yang_variational_2019,
	title = {Variational approximations using Fisher divergence},
	url = {http://arxiv.org/abs/1905.05284},
	abstract = {Modern applications of Bayesian inference involve models that are suﬃciently complex that the corresponding posterior distributions are intractable and must be approximated. The most common approximation is based on Markov chain Monte Carlo, but these can be expensive when the data set is large and/or the model is complex, so more eﬃcient variational approximations have recently received considerable attention. The traditional variational methods, that seek to minimize the Kullback–Leibler divergence between the posterior and a relatively simple parametric family, provide accurate and eﬃcient estimation of the posterior mean, but often does not capture other moments, and have limitations in terms of the models to which they can be applied. Here we propose the construction of variational approximations based on minimizing the Fisher divergence, and develop an eﬃcient computational algorithm that can be applied to a wide range of models without conjugacy or potentially unrealistic mean-ﬁeld assumptions. We demonstrate the superior performance of the proposed method for the benchmark case of logistic regression.},
	number = {{arXiv}:1905.05284},
	publisher = {{arXiv}},
	author = {Yang, Yue and Martin, Ryan and Bondell, Howard},
	urldate = {2022-10-16},
	date = {2019-05-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.05284 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
	file = {Yang et al. - 2019 - Variational approximations using Fisher divergence.pdf:/home/adr/Zotero/storage/F4W9ETZB/Yang et al. - 2019 - Variational approximations using Fisher divergence.pdf:application/pdf},
}

@misc{costa_fisher_2014,
	title = {Fisher information distance: a geometrical reading},
	url = {http://arxiv.org/abs/1210.2354},
	shorttitle = {Fisher information distance},
	abstract = {This paper is a strongly geometrical approach to the Fisher distance, which is a measure of dissimilarity between two probability distribution functions. The Fisher distance, as well as other divergence measures, are also used in many applications to establish a proper data average. The main purpose is to widen the range of possible interpretations and relations of the Fisher distance and its associated geometry for the prospective applications. It focuses on statistical models of the normal probability distribution functions and takes advantage of the connection with the classical hyperbolic geometry to derive closed forms for the Fisher distance in several cases. Connections with the well-known Kullback-Leibler divergence measure are also devised.},
	number = {{arXiv}:1210.2354},
	publisher = {{arXiv}},
	author = {Costa, Sueli I. R. and Santos, Sandra A. and Strapasson, João E.},
	urldate = {2023-09-11},
	date = {2014-01-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1210.2354 [math-ph, stat]},
	keywords = {Computer Science - Information Theory, Mathematical Physics, Statistics - Methodology},
	file = {Costa et al. - 2014 - Fisher information distance a geometrical reading.pdf:/home/adr/Zotero/storage/LKAT8IA7/Costa et al. - 2014 - Fisher information distance a geometrical reading.pdf:application/pdf},
}

@article{neal2011mcmc,
  title={MCMC using Hamiltonian dynamics},
  author={Neal, Radford M and others},
  journal={Handbook of markov chain monte carlo},
  volume={2},
  number={11},
  pages={2},
  year={2011},
  publisher={Chapman and Hall/CRC}
}

@article{tran_tuning_2024,
  title={Tuning diagonal scale matrices for HMC},
  author={Tran, Jimmy Huy and Kleppe, Tore Selland},
  journal={Statistics and Computing},
  volume={34},
  number={6},
  pages={196},
  year={2024},
  publisher={Springer}
}
@incollection{nielsen_riemannian_2013,
	location = {Berlin, Heidelberg},
	title = {The Riemannian Mean of Positive Matrices},
	isbn = {978-3-642-30231-2 978-3-642-30232-9},
	url = {https://link.springer.com/10.1007/978-3-642-30232-9_2},
	pages = {35--51},
	booktitle = {Matrix Information Geometry},
	publisher = {Springer Berlin Heidelberg},
	author = {Bhatia, Rajendra},
	editor = {Nielsen, Frank and Bhatia, Rajendra},
	urldate = {2024-11-26},
	date = {2013},
	langid = {english},
	doi = {10.1007/978-3-642-30232-9_2},
	file = {PDF:/home/adr/Zotero/storage/JF3GN5TD/Bhatia - 2013 - The Riemannian Mean of Positive Matrices.pdf:application/pdf},
}

@misc{dinh_density_2017,
	abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
	annote = {Comment: 10 pages of main content, 3 pages of bibliography, 18 pages of appendix. Accepted at ICLR 2017},
	author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
	doi = {10.48550/arXiv.1605.08803},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	month = feb,
	note = {arXiv:1605.08803 [cs]},
	publisher = {arXiv},
	title = {Density estimation using {Real} {NVP}},
	url = {http://arxiv.org/abs/1605.08803},
	urldate = {2025-03-21},
	year = {2017}
}

@article{carpenter_stan_2017,
  title = {Stan: A Probabilistic Programming Language},
  author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Riddell, Andrew},
  year = {2017},
  journal = {Journal of Statistical Software},
  volume = {76},
  number = {1},
  pages = {1--32},
  doi = {10.18637/jss.v076.i01}
}

@article{hoffman2014no,
  title={The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo.},
  author={Hoffman, Matthew D and Gelman, Andrew},
  journal={J. Mach. Learn. Res.},
  volume={15},
  number={1},
  pages={1593--1623},
  year={2014}
}

@misc{magnusson_posteriordb_2024,
      title={posteriordb: Testing, Benchmarking and Developing Bayesian Inference Algorithms}, 
      author={Måns Magnusson and Jakob Torgander and Paul-Christian Bürkner and Lu Zhang and Bob Carpenter and Aki Vehtari},
      year={2024},
      eprint={2407.04967},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url={https://arxiv.org/abs/2407.04967}, 
}

@misc{pymc_2015,
      title={Probabilistic Programming in Python using PyMC}, 
      author={John Salvatier and Thomas Wiecki and Christopher Fonnesbeck},
      year={2015},
      eprint={1507.08050},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url={https://arxiv.org/abs/1507.08050}, 
}

@misc{hird_quantifying_2024,
      title={Quantifying the effectiveness of linear preconditioning in Markov chain Monte Carlo}, 
      author={Max Hird and Samuel Livingstone},
      year={2024},
      eprint={2312.04898},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url={https://arxiv.org/abs/2312.04898}, 
}

@misc{betancourt_conceptual_2018,
      title={A Conceptual Introduction to Hamiltonian Monte Carlo}, 
      author={Michael Betancourt},
      year={2018},
      eprint={1701.02434},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/1701.02434}, 
}

@article{beskos2013optimal,
  title={Optimal tuning of the hybrid Monte Carlo algorithm},
  author={Beskos, Alexandros and Pillai, Natesh and Roberts, Gareth and Sanz-Serna, Jesus-Maria and Stuart, Andrew},
  year={2013}
}